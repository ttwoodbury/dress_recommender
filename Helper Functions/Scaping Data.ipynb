{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo.errors import DuplicateKeyError\n",
    "from pymongo import MongoClient\n",
    "from pymongo import errors\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mimetypes\n",
    "import requesocks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client.projectData\n",
    "clothes = db.clothes\n",
    "clothes_cols = db.clothesColors\n",
    "item_urls = db.itemUrls\n",
    "clothes_urls = db.clothebsUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = requesocks.session()\n",
    " \n",
    "session.proxies = {\n",
    "    'http': 'socks5://127.0.0.1:9150',\n",
    "    'https': 'socks5://127.0.0.1:9150'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting Forever21 Dresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fov21_data(link):\n",
    "    html = requests.get(link).text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    img_data = soup.select('a[rel*=small]')\n",
    "    img_links = [img_data[i]['rel'][-2].split(',')[0] for i in xrange(len(img_data))]\n",
    "    \n",
    "    \n",
    "    title = soup.select('.product-title')\n",
    "    if title:\n",
    "        title = title[0].text\n",
    "    else:\n",
    "        print 'Title Error at: ', link\n",
    "        return False\n",
    "    \n",
    "    text_price = soup.select('.product-price')\n",
    "    if text_price:\n",
    "        price = float(text_price[0].text.replace('$',\"\"))\n",
    "    else:\n",
    "        new_test_price = soup.select(\".was-now-price\")\n",
    "        if new_test_price:\n",
    "            price = float(new_test_price[0].text.replace('$',\"\"))\n",
    "        else:\n",
    "            print 'Price Error at: ', link\n",
    "            return False\n",
    "    \n",
    "    return {'link': link, 'title': title, 'price': price, 'img_links': img_links, 'source': 'for21'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_colors(product_id):\n",
    "    url = 'http://www.forever21.com/Product/Product.aspx?BR=f21&Category=dress&ProductID=20%s&VariantID='%product_id\n",
    "    html = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'}).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    colors = soup.select('#ctl00_MainContent_upColorList')\n",
    "    if not colors:\n",
    "        return None, None\n",
    "    all_options = colors[0].select('option[value|=%s]'%product_id)\n",
    "    dress_colors = []\n",
    "    color_ids = []\n",
    "    for option in all_options:\n",
    "        dress_colors.append(option.text)\n",
    "        color_ids.append(option['value'][-2:])\n",
    "    return dress_colors, color_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_data(urls, database):\n",
    "    for i, url in enumerate(urls):\n",
    "        bad_links = []\n",
    "        data = get_fov21_data(url)\n",
    "\n",
    "        if not data:\n",
    "            bad_links.append(url)\n",
    "            continue\n",
    "\n",
    "        database.insert_one(data)\n",
    "        if i%20 == 0:\n",
    "            print 'Loaded up to %d' %i\n",
    "        if i%100 == 0:\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_images_to_db(data,dress_images):\n",
    "    for images in dress_images:\n",
    "        dress_id = images['dress_id']\n",
    "        img_urls = images['img_urls']\n",
    "        images = images['images']\n",
    "\n",
    "        data.update({'_id': dress_id}, {'$set': {'img_urls': img_urls, 'img_file_names': images}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = 'http://www.forever21.com/Product/Category.aspx?br=f21&category=dress&pagesize=200&page=%d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a list of the links to the dress pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded up to 20\n",
      "loaded up to 40\n",
      "loaded up to 60\n",
      "loaded up to 80\n",
      "loaded up to 100\n",
      "loaded up to 120\n",
      "loaded up to 140\n",
      "loaded up to 160\n",
      "loaded up to 180\n",
      "loaded up to 200\n",
      "loaded up to 220\n",
      "loaded up to 240\n",
      "loaded up to 260\n",
      "loaded up to 280\n",
      "loaded up to 300\n",
      "loaded up to 320\n",
      "loaded up to 340\n",
      "loaded up to 360\n",
      "loaded up to 380\n",
      "loaded up to 400\n",
      "loaded up to 420\n",
      "loaded up to 440\n",
      "loaded up to 460\n",
      "loaded up to 480\n",
      "loaded up to 500\n",
      "loaded up to 520\n",
      "loaded up to 540\n",
      "loaded up to 560\n",
      "loaded up to 580\n",
      "loaded up to 600\n",
      "loaded up to 620\n",
      "loaded up to 640\n",
      "loaded up to 660\n",
      "loaded up to 680\n",
      "loaded up to 700\n",
      "loaded up to 720\n",
      "loaded up to 740\n",
      "loaded up to 760\n",
      "loaded up to 780\n",
      "loaded up to 800\n",
      "loaded up to 820\n",
      "loaded up to 840\n",
      "loaded up to 860\n",
      "loaded up to 880\n",
      "loaded up to 900\n",
      "loaded up to 920\n",
      "loaded up to 940\n",
      "loaded up to 960\n",
      "loaded up to 980\n",
      "loaded up to 1000\n",
      "loaded up to 1020\n",
      "loaded up to 1040\n",
      "loaded up to 1060\n",
      "loaded up to 1080\n",
      "loaded up to 1100\n",
      "loaded up to 1120\n",
      "loaded up to 1140\n",
      "loaded up to 1160\n",
      "loaded up to 1180\n",
      "loaded up to 1200\n",
      "loaded up to 1220\n",
      "loaded up to 1240\n",
      "loaded up to 1260\n",
      "loaded up to 1280\n",
      "loaded up to 1300\n",
      "loaded up to 1320\n",
      "loaded up to 1340\n",
      "loaded up to 1360\n",
      "loaded up to 1380\n",
      "loaded up to 1400\n",
      "loaded up to 1420\n",
      "loaded up to 1440\n",
      "loaded up to 1460\n",
      "loaded up to 1480\n",
      "loaded up to 1500\n",
      "loaded up to 1520\n",
      "loaded up to 1540\n",
      "loaded up to 1560\n",
      "loaded up to 1580\n",
      "loaded up to 1600\n",
      "loaded up to 1620\n",
      "loaded up to 1640\n",
      "loaded up to 1660\n",
      "loaded up to 1680\n",
      "loaded up to 1700\n",
      "loaded up to 1720\n",
      "loaded up to 1740\n",
      "loaded up to 1760\n",
      "loaded up to 1780\n",
      "loaded up to 1800\n",
      "loaded up to 1820\n",
      "loaded up to 1840\n",
      "loaded up to 1860\n",
      "loaded up to 1880\n",
      "loaded up to 1900\n",
      "loaded up to 1920\n",
      "loaded up to 1940\n",
      "loaded up to 1960\n",
      "loaded up to 1980\n"
     ]
    }
   ],
   "source": [
    "num_pages = 7\n",
    "count = 0\n",
    "for j in xrange(num_pages):\n",
    "    html = session.get(link%(j+1), headers = {'User-agent': 'Mozilla/5.0'}).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    all_urls = soup.select('a[href$=VariantID=]')\n",
    "    for i in xrange(len(all_urls)):\n",
    "        item_urls.insert_one({'url': all_urls[i]['href'], 'scr': 'Fov21'})\n",
    "        count += 1\n",
    "        if count%20 == 0:\n",
    "            print \"loaded up to\", count\n",
    "        if count%100 == 0:\n",
    "            time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for url in item_urls.distinct('url'):\n",
    "    clothes_urls.insert_one({'url': url, 'src': 'fov21'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_types = ['1_front', '2_side', '3_back', '4_full', '5_detail', '6_flat', '7_additional']\n",
    "dress_data = []\n",
    "counter = 0\n",
    "for dress in clothes.find():\n",
    "    img_url = 'http://www.forever21.com/images/%s_330/%s-%s.jpg'\n",
    "    dress_url = dress['link']\n",
    "    product_id = dress_url.split('&')[-2][-8:]\n",
    "    dress_colors, color_ids = get_colors(product_id)\n",
    "    if not dress_colors:\n",
    "        continue\n",
    "    title = dress['title']\n",
    "    price = dress['price']\n",
    "    source = dress['source']\n",
    "    for (color, color_id) in zip (dress_colors, color_ids):\n",
    "        img_urls = []\n",
    "        for view in view_types:\n",
    "            img_urls.append(img_url%(view,product_id,color_id))\n",
    "        \n",
    "        dress_data.append({'link': dress_url, 'title': title, 'price': price, \n",
    "                           'source': source, 'color': color, 'img_urls': img_urls})\n",
    "        \n",
    "    if counter%10 == 0: print \"Loaded %d dresses\" %counter\n",
    "        \n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data in dress_data:\n",
    "    clothes_cols.insert_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 960 dresses total\n",
      "Loaded 3432 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 970 dresses total\n",
      "Loaded 3472 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 980 dresses total\n",
      "Loaded 3517 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 990 dresses total\n",
      "Loaded 3556 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1000 dresses total\n",
      "Loaded 3602 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1010 dresses total\n",
      "Loaded 3648 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1020 dresses total\n",
      "Loaded 3687 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1030 dresses total\n",
      "Loaded 3723 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1040 dresses total\n",
      "Loaded 3761 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1050 dresses total\n",
      "Loaded 3800 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1060 dresses total\n",
      "Loaded 3840 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1070 dresses total\n",
      "Loaded 3881 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1080 dresses total\n",
      "Loaded 3919 images\n",
      "-----------------------------\n",
      "\n",
      "loaded 1090 dresses total\n",
      "Loaded 3955 images\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "image_counter = 0\n",
    "dress_counter = 0\n",
    "\n",
    "for dress in test[dress_counter:]:\n",
    "\n",
    "    good_image_links = []\n",
    "    dress_images = []\n",
    "    for url in dress['img_urls']:   \n",
    "        html = session.get(url)\n",
    "\n",
    "        if html.status_code == 200:\n",
    "            image_counter += 1\n",
    "            good_image_links.append(url)\n",
    "            filename = 'Data/Images/for21%s.png'%image_counter\n",
    "            \n",
    "            if image_counter > 602:\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(html.content)\n",
    "                    \n",
    "            dress_images.append(filename)                \n",
    "\n",
    "            \n",
    "    if not good_image_links:\n",
    "        print \"404 Image Error at: \", dress['link']\n",
    "        continue\n",
    "\n",
    "    dress_counter += 1\n",
    "    \n",
    "    dress['images'] = dress_images\n",
    "    dress['img_urls'] = good_image_links\n",
    "    \n",
    "    images.append({'dress_id': dress['_id'], 'images': dress_images, 'img_urls': good_image_links})\n",
    "\n",
    "    if dress_counter%10 == 0:\n",
    "        print \"loaded %d dresses total\" %dress_counter\n",
    "        print \"Loaded %d images\" %image_counter\n",
    "        print \"-----------------------------\\n\"\n",
    "\n",
    "    if dress_counter%100 ==0:\n",
    "        time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insert_images_to_db(clothes_cols,images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Scrapping Jolly Chic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "with open('~/Documents/Zipfian/project-app/.db_password.txt', 'rb') as f:\n",
    "    PASSWORD = f.readline()\n",
    "conn = psycopg2.connect(\"dbname='postgres' user='Tyler' host='zipfianproject.co9ijljkg9gd.us-east-1.rds.amazonaws.com' port='5432' password=Volkl320\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.jollychic.com/p/ol-style-solid-lace-front-zipper-bodycon-dress-g150599.html#utm_ref=prod_prs-pc_list_one_5'\n",
    "html = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'}).text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    html = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'}).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #Link\n",
    "    link = soup.select('.goods-loading.zoomer_img')[0]['src']\n",
    " \n",
    "    #price\n",
    "    price = float(soup.select('#J-sku-price')[0].text.split()[-1])\n",
    "\n",
    "    #name\n",
    "    name = \" \".join(soup.select('.goods-tlt')[0].text.split()[:-2])\n",
    "\n",
    "    #color\n",
    "    sample = soup.select('.goods-attrs.mt10')[0].text.lower().split()#[1].lower()\n",
    "    test = np.argmax(np.array(sample) == 'color')\n",
    "    color = sample[test+1]\n",
    "\n",
    "    return {'url': url, 'link': link, 'name': name, 'price':price}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = 'http://www.jollychic.com/womens-dresses-c6?jsort=011%d-119'\n",
    "dress_urls = []\n",
    "num_pages = 42\n",
    "for i in xrange(1,num_pages,1):\n",
    "    html = requests.get(page%i, headers = {'User-agent': 'Mozilla/5.0'}).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    all_pages = soup.select('.pro_list_imgbox.categoryTwo-imgbox')\n",
    "        \n",
    "    for page_info in all_pages:\n",
    "        dress_urls.append(page_info.select('a[href]')[0]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 dresses\n",
      "Processed 100 dresses\n",
      "Processed 200 dresses\n",
      "Processed 300 dresses\n",
      "Processed 400 dresses\n",
      "Processed 500 dresses\n",
      "Processed 600 dresses\n",
      "Processed 700 dresses\n",
      "Processed 800 dresses\n",
      "Processed 900 dresses\n",
      "Processed 1000 dresses\n",
      "Processed 1100 dresses\n",
      "Processed 1200 dresses\n",
      "Processed 1300 dresses\n",
      "Processed 1400 dresses\n",
      "Processed 1500 dresses\n",
      "Processed 1600 dresses\n",
      "Processed 1700 dresses\n",
      "Processed 1800 dresses\n",
      "Processed 1900 dresses\n",
      "Processed 2000 dresses\n",
      "Processed 2100 dresses\n",
      "Processed 2200 dresses\n",
      "Processed 2300 dresses\n",
      "Processed 2400 dresses\n",
      "Processed 2500 dresses\n",
      "Processed 2600 dresses\n",
      "Processed 2700 dresses\n",
      "Processed 2800 dresses\n",
      "Processed 2900 dresses\n",
      "Processed 3000 dresses\n",
      "Processed 3100 dresses\n",
      "Processed 3200 dresses\n",
      "Processed 3300 dresses\n",
      "Processed 3400 dresses\n",
      "Processed 3500 dresses\n",
      "Processed 3600 dresses\n",
      "Processed 3700 dresses\n",
      "Processed 3800 dresses\n",
      "Processed 3900 dresses\n",
      "Processed 4000 dresses\n",
      "Processed 4100 dresses\n",
      "Processed 4200 dresses\n",
      "Processed 4300 dresses\n",
      "Processed 4400 dresses\n",
      "Processed 4500 dresses\n",
      "Processed 4600 dresses\n",
      "Processed 4700 dresses\n"
     ]
    }
   ],
   "source": [
    "all_info = []\n",
    "counter = 0\n",
    "for i,url in enumerate(dress_urls):\n",
    "    try:\n",
    "        info = scrape_page(url)\n",
    "    except:\n",
    "        continue\n",
    "    all_info.append(info)\n",
    "    \n",
    "    if counter%100 ==0:\n",
    "        print 'Processed %d dresses'%counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n",
      "Fetched %d images\n"
     ]
    }
   ],
   "source": [
    "fil_names = []\n",
    "counter = 0\n",
    "for i,info in enumerate(all_info):\n",
    "    link = info['link']\n",
    "    \n",
    "    html = requests.get(link)\n",
    "    if html.status_code != 200:\n",
    "        continue\n",
    "        \n",
    "    name = 'Data/Images/jolly%s.jpg'%i\n",
    "    fil_names.append(name)\n",
    "    info['file_name'] = name\n",
    "    with open(name,'wb') as f:\n",
    "        f.write(html.content)\n",
    "    if counter%100 == 0:\n",
    "        print 'Fetched %d images'\n",
    "    counter += 1\n",
    "\n",
    "all_info2 = [info for info in all_info if 'file_name' in info]\n",
    "pickle.dump(all_info2,open('new_imgs.plk','wb'),pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
